{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinir pd.read_csv e pd.concat para as funções originais\n",
    "pd.read_csv = pd.io.parsers.read_csv\n",
    "pd.concat = pd.core.reshape.concat.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = \"C:/Mafaulda/normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_csv = glob.glob(os.path.join(normal, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Carregar e concatenar todos os arquivos CSV em um único DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_normal \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marquivos_csv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Exibir as primeiras linhas do DataFrame resultante\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_normal\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Carlos Carneiro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Carlos Carneiro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 443\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    446\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\Carlos Carneiro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    502\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Carregar e concatenar todos os arquivos CSV em um único DataFrame\n",
    "df_normal = pd.concat((pd.read_csv(f) for f in arquivos_csv), ignore_index=True)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame resultante\n",
    "print(df_normal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho data_dir:\", len(normal))\n",
    "print(\"Tamanho do dataset:\", df_normal.shape)\n",
    "print(\"--------------------\")\n",
    "df_normal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_normal.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalhando o arquivo Normal 28.8768.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o arquivo CSV específico\n",
    "arquivo_01_csv = \"c:/Mafaulda/normal/28.8768.csv\"\n",
    "\n",
    "# Carregar o arquivo CSV específico\n",
    "df_arquivo_01_csv = pd.read_csv(arquivo_01_csv)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame resultante\n",
    "print(df_arquivo_01_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colunas:\n",
    "\n",
    "- column 1: tachometer signal that allows to estimate rotation frequency - TS;\n",
    "\n",
    "- columns 2 to 4: underhang bearing accelerometer - UBA (axial, radiale tangential direction);\n",
    "\n",
    "- columns 5 to 7: overhang bearing accelerometer - OBA (axial, radiale tangential direction);\n",
    "\n",
    "- column 8 - microphone - MIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_colunas = ['TS', 'UBA_01', 'UBA_02', 'UBA_03', 'OBA_01', 'OBA_02', 'OBA_03', 'MIC']\n",
    "arquivo_01_csv = \"c:/Mafaulda/normal/28.8768.csv\"\n",
    "df_arquivo_01_csv = pd.read_csv(arquivo_01_csv, header=None, names=nomes_colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_arquivo_01_csv.head())\n",
    "print(df_arquivo_01_csv.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_arquivo_01_csv['TS'].hist(bins=30)\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Distribuição de Frequências')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arquivo_01_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arquivo_01_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arquivo_01_csv.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(8, 12))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(df_arquivo_01_csv.columns[:8]):  # Limitando às primeiras 9 colunas\n",
    "    df_arquivo_01_csv[col].hist(ax=axes[i], bins=40)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "#df_arquivo_01_csv.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abordando o problema de forma não supervisionada:\n",
    "\n",
    "- Uso de autoencoder tanto no SBM como no AAKR, adapatados para funcionar como modelos de aprendizado não supervisionado. Prevendo padrões ou anomalias nos dados (análise auto-associativa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_arquivo_01_csv\n",
    "\n",
    "X = df[['TS', 'UBA_01', 'UBA_02', 'UBA_03', 'OBA_01', 'OBA_02', 'OBA_03', 'MIC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de dados para X (substitua pelos seus dados reais)\n",
    "X = np.random.rand(40000, 1)  # Exemplo de 1000 amostras, 1 característica\n",
    "\n",
    "X = X.reshape((-1, 1))\n",
    "\n",
    "# Separação desenvolvimento (30) e teste (70)\n",
    "X_dev, X_test = train_test_split(X, test_size=0.8, random_state=42)\n",
    "\n",
    "# Separação treino (80) e validação (20) - Hold-out\n",
    "X_train, X_val, = train_test_split(X_dev, test_size=0.8, random_state=42)\n",
    "\n",
    "# Normalizando dados para treino, validação e teste\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Exibir as formas dos conjuntos de dados\n",
    "print(\"Shape do conjunto de treino:\", X_train.shape)\n",
    "print(\"Shape do conjunto de validação:\", X_val.shape)\n",
    "print(\"Shape do conjunto de teste:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Autoencoder simples para SBM\n",
    "def create_sbm_autoencoder(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        Dropout(0.30),  # Dropout após a primeira camada\n",
    "        Dense(32, activation='relu'),  # Encoder\n",
    "        Dropout(0.30),  # Dropout após o Encoder\n",
    "        Dense(64, activation='relu'),  # Decoder\n",
    "        Dropout(0.30),  # Dropout após o Decoder\n",
    "        Dense(input_shape)  # Saída de mesma dimensão que a entrada\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')  # Usar MAE para medir o erro\n",
    "    return model\n",
    "\n",
    "\n",
    "# Callback personalizado para acompanhar a acurácia\n",
    "class AccuracyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        accuracy = 1 - loss\n",
    "        val_accuracy = 1 - val_loss\n",
    "        print(f\"Epoch {epoch+1}: Perda Treinamento: {loss:.4f}, Acurácia Treinamento: {accuracy:.4f}, Perda Validação: {val_loss:.4f}, Acurácia Validação: {val_accuracy:.4f}\")\n",
    "        \n",
    "        \n",
    "\n",
    "# Criar o autoencoder e atribuir à variável 'sbm_autoencoder'\n",
    "sbm_autoencoder = create_sbm_autoencoder(X_train.shape[1])\n",
    "\n",
    "# Exibir o sumário do modelo\n",
    "sbm_autoencoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A acurácia de um autoencoder geralmente é medida com base no erro de reconstrução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treinar o modelo SBM e armazenar o histórico de treinamento\n",
    "history = sbm_autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[AccuracyCallback()],  # \"AccuracyCallback\" com 'C' maiúsculo\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fazer previsões com o Autoencoder SBM\n",
    "X_pred_sbm = sbm_autoencoder.predict(X_test)\n",
    "sbm_value = np.mean(np.abs(X_test - X_pred_sbm))  # Erro médio absoluto como SBM\n",
    "\n",
    "# Calcular \"acurácia\" inversamente proporcional ao erro de reconstrução (quanto menor o erro, melhor a acurácia)\n",
    "accuracy = 1 - sbm_value\n",
    "\n",
    "# Exibir resultado SBM\n",
    "print(f\"SBM (MSE - Erro de Reconstrução): {sbm_value:.4f}\")\n",
    "print(f\"Acurácia (1 - Erro de Reconstrução): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar curvas de perda (training e validation loss)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Curvas de Perda (Training vs Validation)')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda (Loss)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Adicionar curvas de \"acurácia\" baseada no erro de reconstrução (quanto menor o erro, maior a acurácia)\n",
    "train_accuracy = 1 - np.array(history.history['loss'])\n",
    "val_accuracy = 1 - np.array(history.history['val_loss'])\n",
    "\n",
    "# Plotar curvas de \"acurácia\"\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracy, label='Training Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Curvas de Acurácia (Baseada no Erro de Reconstrução)')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
